{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "import pprint\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import HuberRegressor, RANSACRegressor, TheilSenRegressor, Lasso, ElasticNet, Ridge, LassoCV, ElasticNetCV, RidgeCV, LinearRegression, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "import statsmodels.formula.api as sm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import silhouette_score, completeness_score, homogeneity_score, v_measure_score, adjusted_mutual_info_score\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import cufflinks as cf\n",
    "from timeit import default_timer as timer\n",
    "# REQUIREMENTS:\n",
    "# pip install pydotplus\n",
    "# brew install graphviz\n",
    "\n",
    "# Use graphviz to make a chart of the regression tree decision points:\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "import kaggle\n",
    "\n",
    "### NLP\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from gensim import corpora, models, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "### Ways to load stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = pd.read_csv('age_gender_bkts.csv')\n",
    "cty = pd.read_csv('countries.csv')\n",
    "sample_submission = pd.read_csv('sample_submission_NDF.csv')\n",
    "sessions = pd.read_csv('sessions.csv')\n",
    "df_test = pd.read_csv('test_users.csv')\n",
    "df_train = pd.read_csv('train_users_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10567737, 6), (213451, 16), (62096, 15))"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions.shape, df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_account_created</th>\n",
       "      <th>timestamp_first_active</th>\n",
       "      <th>date_first_booking</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>signup_method</th>\n",
       "      <th>signup_flow</th>\n",
       "      <th>language</th>\n",
       "      <th>affiliate_channel</th>\n",
       "      <th>affiliate_provider</th>\n",
       "      <th>first_affiliate_tracked</th>\n",
       "      <th>signup_app</th>\n",
       "      <th>first_device_type</th>\n",
       "      <th>first_browser</th>\n",
       "      <th>country_destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gxn3p5htnn</td>\n",
       "      <td>2010-06-28</td>\n",
       "      <td>20090319043255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>facebook</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>820tgsjxq7</td>\n",
       "      <td>2011-05-25</td>\n",
       "      <td>20090523174809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MALE</td>\n",
       "      <td>38.0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>seo</td>\n",
       "      <td>google</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4ft3gnwmtx</td>\n",
       "      <td>2010-09-28</td>\n",
       "      <td>20090609231247</td>\n",
       "      <td>2010-08-02</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>56.0</td>\n",
       "      <td>basic</td>\n",
       "      <td>3</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Windows Desktop</td>\n",
       "      <td>IE</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bjjt8pjhuk</td>\n",
       "      <td>2011-12-05</td>\n",
       "      <td>20091031060129</td>\n",
       "      <td>2012-09-08</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>42.0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87mebub9p4</td>\n",
       "      <td>2010-09-14</td>\n",
       "      <td>20091208061105</td>\n",
       "      <td>2010-02-18</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>41.0</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id date_account_created  timestamp_first_active date_first_booking  \\\n",
       "0  gxn3p5htnn           2010-06-28          20090319043255                NaN   \n",
       "1  820tgsjxq7           2011-05-25          20090523174809                NaN   \n",
       "2  4ft3gnwmtx           2010-09-28          20090609231247         2010-08-02   \n",
       "3  bjjt8pjhuk           2011-12-05          20091031060129         2012-09-08   \n",
       "4  87mebub9p4           2010-09-14          20091208061105         2010-02-18   \n",
       "\n",
       "      gender   age signup_method  signup_flow language affiliate_channel  \\\n",
       "0  -unknown-   NaN      facebook            0       en            direct   \n",
       "1       MALE  38.0      facebook            0       en               seo   \n",
       "2     FEMALE  56.0         basic            3       en            direct   \n",
       "3     FEMALE  42.0      facebook            0       en            direct   \n",
       "4  -unknown-  41.0         basic            0       en            direct   \n",
       "\n",
       "  affiliate_provider first_affiliate_tracked signup_app first_device_type  \\\n",
       "0             direct               untracked        Web       Mac Desktop   \n",
       "1             google               untracked        Web       Mac Desktop   \n",
       "2             direct               untracked        Web   Windows Desktop   \n",
       "3             direct               untracked        Web       Mac Desktop   \n",
       "4             direct               untracked        Web       Mac Desktop   \n",
       "\n",
       "  first_browser country_destination  \n",
       "0        Chrome                 NDF  \n",
       "1        Chrome                 NDF  \n",
       "2            IE                  US  \n",
       "3       Firefox               other  \n",
       "4        Chrome                  US  "
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_account_created</th>\n",
       "      <th>timestamp_first_active</th>\n",
       "      <th>date_first_booking</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>signup_method</th>\n",
       "      <th>signup_flow</th>\n",
       "      <th>language</th>\n",
       "      <th>affiliate_channel</th>\n",
       "      <th>affiliate_provider</th>\n",
       "      <th>first_affiliate_tracked</th>\n",
       "      <th>signup_app</th>\n",
       "      <th>first_device_type</th>\n",
       "      <th>first_browser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5uwns89zht</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>20140701000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>35.0</td>\n",
       "      <td>facebook</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Moweb</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Mobile Safari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jtl0dijy2j</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>20140701000051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Moweb</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Mobile Safari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xx0ulgorjt</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>20140701000148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>linked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Windows Desktop</td>\n",
       "      <td>Chrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6c6puo6ix0</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>20140701000215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>linked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Windows Desktop</td>\n",
       "      <td>IE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>czqhjk3yfe</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>20140701000305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-unknown-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>untracked</td>\n",
       "      <td>Web</td>\n",
       "      <td>Mac Desktop</td>\n",
       "      <td>Safari</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id date_account_created  timestamp_first_active  \\\n",
       "0  5uwns89zht           2014-07-01          20140701000006   \n",
       "1  jtl0dijy2j           2014-07-01          20140701000051   \n",
       "2  xx0ulgorjt           2014-07-01          20140701000148   \n",
       "3  6c6puo6ix0           2014-07-01          20140701000215   \n",
       "4  czqhjk3yfe           2014-07-01          20140701000305   \n",
       "\n",
       "   date_first_booking     gender   age signup_method  signup_flow language  \\\n",
       "0                 NaN     FEMALE  35.0      facebook            0       en   \n",
       "1                 NaN  -unknown-   NaN         basic            0       en   \n",
       "2                 NaN  -unknown-   NaN         basic            0       en   \n",
       "3                 NaN  -unknown-   NaN         basic            0       en   \n",
       "4                 NaN  -unknown-   NaN         basic            0       en   \n",
       "\n",
       "  affiliate_channel affiliate_provider first_affiliate_tracked signup_app  \\\n",
       "0            direct             direct               untracked      Moweb   \n",
       "1            direct             direct               untracked      Moweb   \n",
       "2            direct             direct                  linked        Web   \n",
       "3            direct             direct                  linked        Web   \n",
       "4            direct             direct               untracked        Web   \n",
       "\n",
       "  first_device_type  first_browser  \n",
       "0            iPhone  Mobile Safari  \n",
       "1            iPhone  Mobile Safari  \n",
       "2   Windows Desktop         Chrome  \n",
       "3   Windows Desktop             IE  \n",
       "4       Mac Desktop         Safari  "
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distinct_action_timescores(df_input, sessions, optimizing_class ='NDF'):\n",
    "    \"\"\"\n",
    "        Function that takes in\n",
    "            df_input as a DataFrame of train data\n",
    "            sessions as a DataFrame of sessions information\n",
    "            optimizing_class as a string of the class to look out for\n",
    "        \n",
    "        Outputs a dictionary of the absolute time diff between class yes and no and its action \n",
    "    \"\"\"\n",
    "    df = df_input.copy(deep=True)\n",
    "    \n",
    "    df['optimizing_class_boolean'] = [1 if val == optimizing_class else 0 for val in df['country_destination']]\n",
    "    \n",
    "    dict_timescore = {}\n",
    "    timescore_list = []\n",
    "    start = timer()\n",
    "\n",
    "    for action in sessions['action'].value_counts().index[:75]:\n",
    "        print ('>', end='')\n",
    "        action_sessions = sessions[sessions['action']==action].groupby('user_id').sum()['secs_elapsed']\n",
    "        df_holding = df.merge(action_sessions.reset_index(), how='left', left_on='id', right_on='user_id')\n",
    "        \n",
    "        time_array = df_holding.groupby(['optimizing_class_boolean']).mean()['secs_elapsed'] \n",
    "        timescore = time_array[1] - time_array[0]\n",
    "        \n",
    "        dict_timescore[abs(timescore)] = [action, timescore]\n",
    "        timescore_list += [abs(timescore)]\n",
    "        \n",
    "    end = timer()\n",
    "    print (' {} seconds elapsed...'.format(end-start))\n",
    "\n",
    "    return dict_timescore, timescore_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create X, y, and test sets for each of the location classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_set(train, test, sessions, timescores_info, timescore_limit=10):\n",
    "    \n",
    "    train_set = train.copy(deep=True)\n",
    "    test_set = test.copy(deep=True)\n",
    "    \n",
    "    df_dictionary = {}\n",
    "    \n",
    "    \n",
    "    for location_class in train_set['country_destination'].unique():\n",
    "        start = timer()\n",
    "        \n",
    "        print ('Start {} > '.format(location_class), end='' )\n",
    "        ### Get the timescore_list and dict_timescore for location class\n",
    "        timescore_list = timescores_info[location_class][0]\n",
    "        dict_timescore = timescores_info[location_class][1]\n",
    "        \n",
    "        print ('Getting Actions > ', end='' )\n",
    "        ### Get top 10 timescore actions\n",
    "        action_list = get_top_actions(timescore_list, dict_timescore, timescore_limit)\n",
    "        \n",
    "        print ('Aggregating train/test sets > ', end='' )\n",
    "        ### Aggregate train and test sets with second elapsed for actions\n",
    "        agg_train = agg_action_with_data(train_set, sessions, action_list).copy(deep=True)\n",
    "        agg_test = agg_action_with_data(test_set, sessions, action_list).copy(deep=True)\n",
    "        \n",
    "        print ('Justifying Columns > ', end='' )\n",
    "        ### Justify the columns \n",
    "        ### Fill agg_test with columns unique only to agg_train, values = 0 \n",
    "        for column in agg_train.columns:\n",
    "            if column not in agg_test.columns:\n",
    "                agg_test[column] = 0 \n",
    "\n",
    "        ### Fill agg_train with columns unique only to agg_test, values = 0\n",
    "        for column in agg_test.columns:\n",
    "            if column not in agg_train.columns:\n",
    "                agg_train[column] = 0 \n",
    "        \n",
    "        ### Align agg_test columns with agg_train columns \n",
    "        agg_test = agg_test[agg_train.columns]\n",
    "        \n",
    "        print ('Saving... > ', end='' )\n",
    "        ### Store information in massive dictionary called df_dictionary\n",
    "        df_dictionary[location_class] = [agg_train.copy(deep=True), agg_test.copy(deep=True)]\n",
    "        \n",
    "        end = timer()\n",
    "        print ('Completed {}, {} seconds elapsed'.format(location_class, end-start))\n",
    "        \n",
    "    return df_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_top_actions(timescore_list, dict_timescore, actions=10):\n",
    "    \"\"\"\n",
    "        Function that takes in;\n",
    "            timescore_list as a list, ordered timescores\n",
    "            dict_timescore as a dictionary, with the timescores as keys and the action & non-abs-timescores as values\n",
    "            action as int, number of actiosn to grab\n",
    "            \n",
    "        Returns a list of the top # actions\n",
    "    \"\"\"\n",
    "    \n",
    "    action_list = []\n",
    "    \n",
    "    for timescore in timescore_list[:actions]:\n",
    "        \n",
    "        action_list += [dict_timescore[timescore][0]]\n",
    "        \n",
    "    return action_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def agg_action_with_data(data, sessions, action_list):\n",
    "    \"\"\"\n",
    "        Funtion that takes in;\n",
    "            data as DataFrame, base dataset to becombined\n",
    "            sessions as DataFrame, sessions log information\n",
    "            action_list as list, the list of actions to be added\n",
    "            \n",
    "        Add the avg time spent for each action in the action_list for each user to the main dataset\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    data_out = data.copy(deep=True)\n",
    "    for action in action_list:\n",
    "\n",
    "        session_action = sessions[sessions['action']==action].groupby('user_id').mean()[['secs_elapsed']]\n",
    "        data_out = data_out.merge(session_action, how='left', left_index=True, right_index=True)\n",
    "        data_out.rename(mapper={'secs_elapsed':action + '_secs_elapsed'}, axis=1, inplace=True)\n",
    "\n",
    "    ### Clean the new columns and deal with nulls by converting them to 0\n",
    "    for action in action_list:\n",
    "        data_out[action +'_secs_elapsed'].fillna(value=0, inplace=True)\n",
    "        data_out[action +'_secs_elapsed'] = data_out[action +'_secs_elapsed'].apply(lambda val: int(np.round(val, 0)))\n",
    "        \n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_many_models(massive_df_dictionary, location_classes, on_dataset='test'):\n",
    "    \"\"\"\n",
    "        takes in; \n",
    "            massive_df_dictionary as a dictionary of DataFrames\n",
    "            location_classes as a list of location_classes\n",
    "            \n",
    "        Run a RandomForest model through each of it and saves down the predict_proba for each of it.\n",
    "    \"\"\"\n",
    "    if on_dataset == 'test':\n",
    "        df_out = pd.DataFrame(index=massive_df_dictionary['NDF'][1].index.values)\n",
    "    elif on_dataset == 'train':\n",
    "        df_out = pd.DataFrame(index=massive_df_dictionary['NDF'][0].index.values)\n",
    "    \n",
    "    for location_class in location_classes:\n",
    "        start = timer()\n",
    "        \n",
    "        print ('Start {} > '.format(location_class), end ='')\n",
    "        train = massive_df_dictionary[location_class][0]\n",
    "        test = massive_df_dictionary[location_class][1]\n",
    "        \n",
    "        print ('Ready data for modeling > ', end='')\n",
    "        X_train = train.drop(columns=['country_destination']).copy(deep=True)\n",
    "        y_train = [1 if val == location_class else 0 for val in train['country_destination']]\n",
    "        \n",
    "        if on_dataset == 'test':\n",
    "            X_test = test.drop(columns=['country_destination']).copy(deep=True)\n",
    "        elif on_dataset == 'train':\n",
    "            X_test = X_train\n",
    "        \n",
    "        print ('Fitting Model > ', end='')\n",
    "        rdf = RandomForestClassifier(n_jobs=10, n_estimators=100, random_state=0, class_weight='balanced')\n",
    "        rdf.fit(X_train, y_train)\n",
    "        \n",
    "        print ('Getting Probas > ', end='')\n",
    "        pred_proba = pd.DataFrame(rdf.predict_proba(X_test), index=X_test.index.values, columns=rdf.classes_)\n",
    "        pred_proba.rename(mapper={1: location_class + '_Y', 0: location_class + '_N'}, axis = 1, inplace=True)\n",
    "        \n",
    "        print ('Saving > ', end='')\n",
    "        df_out = df_out.merge(pred_proba, how='left', left_index=True, right_index=True)\n",
    "        \n",
    "        end = timer()\n",
    "        print ('Done {}, {} secconds'.format(location_class, np.round(end-start)))\n",
    "        \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Find top choices function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_choices(df, threshold = 0.9, max_picks=5):\n",
    "    holding_list = []\n",
    "    \n",
    "    for row in df.index:\n",
    "        sum_prob = 0\n",
    "        prob_dict = {}\n",
    "        prob_list = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            prob = df.loc[row, col]\n",
    "            prob_dict[prob] = col\n",
    "            prob_list += [prob]\n",
    "            \n",
    "        prob_list.sort(reverse=True)\n",
    "        \n",
    "        thres_checker = 0\n",
    "        for proba in prob_list[:max_picks]:\n",
    "            if proba == 0:\n",
    "                break\n",
    "            \n",
    "            holding_list += [[row, prob_dict[proba]]]\n",
    "            thres_checker += proba\n",
    "            if thres_checker > threshold:\n",
    "                break\n",
    "                \n",
    "    output_df = pd.DataFrame(holding_list, columns=['id', 'country'])\n",
    "    output_df.drop_duplicates(inplace=True)\n",
    "    output_df.set_index(keys='id', inplace=True)\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Submit to kaggle\n",
    "def submit_to_kaggle(output, filename, message='null'):\n",
    "    \n",
    "    if type(output) != pd.core.frame.DataFrame:\n",
    "        file = pd.DataFrame(output, columns=['country'], index=kaggle_test.index)\n",
    "    else:\n",
    "        file = output.copy(deep=True)\n",
    "    file.to_csv(path_or_buf=filename + '.csv')\n",
    "    kaggle.api.competition_submit(file_name=filename + '.csv', message=message, competition= 'airbnb-recruiting-new-user-bookings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create encoder to encoder y values to use neural network\n",
    "import copy\n",
    "def ANN_encoder(y): \n",
    "    \n",
    "    location_classes = y.unique()\n",
    "    encoder = {}\n",
    "    \n",
    "    for i, location_class in enumerate(location_classes):\n",
    "        encoder[location_class] = i\n",
    "    \n",
    "    output_array = []\n",
    "    \n",
    "    for location in y:\n",
    "        builder = []\n",
    "        for i in range(len(location_classes)):\n",
    "            if i == encoder[location]:\n",
    "                builder += [1]\n",
    "                \n",
    "            else:\n",
    "                builder += [0]\n",
    "                \n",
    "        output_array += [copy.deepcopy(builder)]\n",
    "    \n",
    "    decoder = {encoder[key] : key for key in encoder.keys()}\n",
    "        \n",
    "    return decoder, np.array(output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deal with the age part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213451"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_age = df_train.copy(deep=True)\n",
    "\n",
    "### using date of account creation as current date, calculate the age for those with inputs as number of years\n",
    "aegis = []\n",
    "for date, age in zip(df_age['date_account_created'], df_age['age']):\n",
    "    if np.isnan(age) == False:\n",
    "        if age >= 1000:\n",
    "            aegis += [float(date[:4]) -age]\n",
    "        else:\n",
    "            aegis += [age]\n",
    "    else:\n",
    "        aegis += [age]\n",
    "        \n",
    "### Add the first order treated age back to the training set\n",
    "df_age['age'] = aegis\n",
    "\n",
    "### Turn anyone below 10years old into np.nan for easy treatment\n",
    "df_age[(df_age['age'] <= 10)]['age'] = np.nan\n",
    "\n",
    "### Turn anyone above 100years old into np.nan for easy treatment\n",
    "df_age[(df_age['age'] > 100)]['age'] = np.nan\n",
    "\n",
    "### Get the age mean of people within sane range of above 10 to 100 in the training set, rest are considered errors to be treated\n",
    "sane_age_mean = np.round(df_age[(df_age['age'] > 10)  &  (df_age['age'] <= 100)]['age'].mean(), 0)\n",
    "\n",
    "### Fill the np.nans with the calculated mean age\n",
    "df_age['age'] = df_age['age'].fillna(sane_age_mean)\n",
    "\n",
    "\n",
    "# df_age[(df_age['age'] > 100)]\n",
    "df_age['age'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treating null values & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213451, 149)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_age.copy(deep=True)\n",
    "\n",
    "### Set ids as index\n",
    "df.reset_index(inplace=True)\n",
    "df.set_index(df['id'], inplace=True)\n",
    "\n",
    "### Drop data_first_booking as it is not available in the test set \n",
    "df.drop(columns=['date_first_booking'], inplace=True)\n",
    "\n",
    "### Fill np.nan of 'first_affiliate_tracked' information with 'untracked'\n",
    "df['first_affiliate_tracked'] = df['first_affiliate_tracked'].fillna('untracked')\n",
    "\n",
    "\n",
    "# ### Create new time of the day columns using timestamp_first_active column\n",
    "# df['tod_first_active'] = [str(time)[4:6] for time in df['timestamp_first_active']]\n",
    "\n",
    "\n",
    "### Drop columns that are not useful in helping to predict\n",
    "df.drop(columns=['index','id', 'date_account_created', 'timestamp_first_active'], inplace=True)\n",
    "\n",
    "df = pd.get_dummies(df, columns=['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDF >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 77.10597657971084 seconds elapsed...\n",
      "getting actions from timescores...\n",
      "US >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 76.42651872462011 seconds elapsed...\n",
      "getting actions from timescores...\n",
      "other >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 76.86634620290715 seconds elapsed...\n",
      "getting actions from timescores...\n",
      "FR >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 77.65868799999589 seconds elapsed...\n",
      "getting actions from timescores...\n",
      "CA >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 76.46094979709596 seconds elapsed...\n",
      "getting actions from timescores...\n",
      "GB >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 76.29486840579193 seconds elapsed...\n",
      "getting actions from timescores...\n",
      "ES >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 76.88173402898246 seconds elapsed...\n",
      "getting actions from timescores...\n",
      "IT >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 76.59943976812065 seconds elapsed...\n",
      "getting actions from timescores...\n",
      "PT >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 76.54265043479973 seconds elapsed...\n",
      "getting actions from timescores...\n",
      "NL >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 76.51306527538691 seconds elapsed...\n",
      "getting actions from timescores...\n",
      "DE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 76.6603380869783 seconds elapsed...\n",
      "getting actions from timescores...\n",
      "AU >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> 76.20195292754215 seconds elapsed...\n",
      "getting actions from timescores...\n"
     ]
    }
   ],
   "source": [
    "### Find the time diff for the first 75 actions for each location class\n",
    "### Gather the top 10 actions for each of the location classes\n",
    "\n",
    "info_dict = {}\n",
    "top10_cumulative =set()\n",
    "\n",
    "for location_class in df_train['country_destination'].unique():\n",
    "    \n",
    "    print (location_class, end=' ')\n",
    "    dict_timescore, timescore_list = find_distinct_action_timescores(df_train, sessions, location_class)\n",
    "    timescore_list.sort(reverse=True)\n",
    "    info_dict[location_class] = [timescore_list, dict_timescore]\n",
    "    \n",
    "    print ('getting actions from timescores...')\n",
    "    for timescore in timescore_list[:10]:\n",
    "        top10_cumulative.update([dict_timescore[timescore][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Aggregating & merging train set and session information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy(deep=True)\n",
    "\n",
    "### Add in the avg time spend on each action based on those with the highest difference between those who flew and those who didn't\n",
    "### Add average secs_elapsed information for each action to the main training set\n",
    "for action in top10_cumulative:\n",
    "    \n",
    "    session_action = sessions[sessions['action']==action].groupby('user_id').mean()[['secs_elapsed']]\n",
    "    df1 = df1.merge(session_action, how='left', left_index=True, right_index=True)\n",
    "    df1.rename(mapper={'secs_elapsed':action + '_secs_elapsed'}, axis=1, inplace=True)\n",
    "    \n",
    "### Clean the new columns and deal with nulls by converting them to 0\n",
    "for action in top10_cumulative:\n",
    "    df1[action +'_secs_elapsed'].fillna(value=0, inplace=True)\n",
    "    df1[action +'_secs_elapsed'] = df1[action +'_secs_elapsed'].apply(lambda val: int(np.round(val, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Create a function to do the same treatment for df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def treat_test(test, sane_age_mean):\n",
    "    \n",
    "    df_age = test.copy(deep=True)\n",
    "\n",
    "    ### using date of account creation as current date, calculate the age for those with inputs as number of years\n",
    "    aegis = []\n",
    "    for date, age in zip(df_age['date_account_created'], df_age['age']):\n",
    "        if np.isnan(age) == False:\n",
    "            if age >= 1000:\n",
    "                aegis += [float(date[:4]) -age]\n",
    "            else:\n",
    "                aegis += [age]\n",
    "        else:\n",
    "            aegis += [age]\n",
    "\n",
    "    ### Add the first order treated age back to the training set\n",
    "    df_age['age'] = aegis\n",
    "\n",
    "    ### Turn anyone below 10years old into np.nan for easy treatment\n",
    "    df_age[(df_age['age'] <= 10)]['age'] = np.nan\n",
    "\n",
    "    ### Turn anyone above 100years old into np.nan for easy treatment\n",
    "    df_age[(df_age['age'] > 100)]['age'] = np.nan\n",
    "\n",
    "    ### Fill the np.nans with the calculated mean age\n",
    "    df_age['age'] = df_age['age'].fillna(sane_age_mean)\n",
    "\n",
    "    df = df_age.copy(deep=True)\n",
    "\n",
    "    ### Set ids as index\n",
    "    df.reset_index(inplace=True)\n",
    "    df.set_index(df['id'], inplace=True)\n",
    "\n",
    "    ### Drop data_first_booking as it is not available in the test set \n",
    "    df.drop(columns=['date_first_booking'], inplace=True)\n",
    "\n",
    "    ### Fill np.nan of 'first_affiliate_tracked' information with 'untracked'\n",
    "    df['first_affiliate_tracked'] = df['first_affiliate_tracked'].fillna('untracked')\n",
    "\n",
    "\n",
    "    # ### Create new time of the day columns using timestamp_first_active column\n",
    "    # df['tod_first_active'] = [str(time)[4:6] for time in df['timestamp_first_active']]\n",
    "\n",
    "\n",
    "    ### Drop columns that are not useful in helping to predict\n",
    "    df.drop(columns=['index','id', 'date_account_created', 'timestamp_first_active'], inplace=True)\n",
    "\n",
    "    ### Create dummies for categorical features\n",
    "    df = pd.get_dummies(df, columns=['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62096, 183), (213451, 183))"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_cleaned = treat_test(df_test, sane_age_mean)\n",
    "\n",
    "\n",
    "for action in top10_cumulative:\n",
    "    \n",
    "    session_action = sessions[sessions['action']==action].groupby('user_id').mean()[['secs_elapsed']]\n",
    "    df_test_cleaned = df_test_cleaned.merge(session_action, how='left', left_index=True, right_index=True)\n",
    "    df_test_cleaned.rename(mapper={'secs_elapsed':action + '_secs_elapsed'}, axis=1, inplace=True)\n",
    "    \n",
    "### Clean the new columns and deal with nulls by converting them to 0\n",
    "for action in top10_cumulative:\n",
    "    df_test_cleaned[action +'_secs_elapsed'].fillna(value=0, inplace=True)\n",
    "    df_test_cleaned[action +'_secs_elapsed'] = df_test_cleaned[action +'_secs_elapsed'].apply(lambda val: int(np.round(val, 0)))\n",
    "\n",
    "\n",
    "for column in df1.columns:\n",
    "    if column not in df_test_cleaned.columns:\n",
    "        df_test_cleaned[column] = 0 \n",
    "        \n",
    "for column in df_test_cleaned.columns:\n",
    "    if column not in df1.columns:\n",
    "        df1[column] = 0 \n",
    "        \n",
    "df_test_cleaned = df_test_cleaned[df1.columns]\n",
    "\n",
    "df_test_cleaned.shape, df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ready data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning:\n",
      "\n",
      "Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "\n",
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning:\n",
      "\n",
      "Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "\n",
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning:\n",
      "\n",
      "Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "\n",
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning:\n",
      "\n",
      "Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Ready data for modeling\n",
    "X = df1.drop(columns=['country_destination']).copy(deep=True)\n",
    "y = df1['country_destination']\n",
    "\n",
    "### Create X for test set from kaggle\n",
    "kaggle_test = df_test_cleaned.drop(columns=['country_destination']).copy(deep=True)\n",
    "\n",
    "### Create a standardized version for the models \n",
    "ss = StandardScaler()\n",
    "Xs = pd.DataFrame(ss.fit_transform(X), index = X.index, columns = X.columns)\n",
    "kaggle_test_std = pd.DataFrame(ss.fit_transform(kaggle_test), index = kaggle_test.index, columns = kaggle_test.columns)\n",
    "\n",
    "# y = [1 if val == 'NDF' else 0 for val in y]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.10, random_state=0)\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, y, test_size=.10, random_state=0)\n",
    "\n",
    "### Encoder ys to be used for neural networks\n",
    "decoder_yNN, yNN = ANN_encoder(y)\n",
    "decoder_yNN_train, yNN_train = ANN_encoder(ys_train)\n",
    "decoder_yNN_test, yNN_test = ANN_encoder(ys_test)\n",
    "\n",
    "for train, test in zip(y_train.values, ys_train.values):\n",
    "    if train != test:\n",
    "        print ('gg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=10, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Try baseline random forest\n",
    "rdf = RandomForestClassifier(n_jobs=10, n_estimators=100, random_state=0, class_weight='balanced')\n",
    "rdf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6872013491989132"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 960k/960k [00:07<00:00, 136kB/s]\n"
     ]
    }
   ],
   "source": [
    "submit_to_kaggle(rdf.predict(kaggle_test), 'first_try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3.87M/3.87M [00:09<00:00, 411kB/s]\n"
     ]
    }
   ],
   "source": [
    "### Doesn't seem very good at predicting when doing train test split, though it seems to yield a not too bad first score at kaggle of 0.69\n",
    "### Try submitting more guesses for each user\n",
    "rdf_more_guesses = find_top_choices(pd.DataFrame(rdf.predict_proba(kaggle_test), columns=rdf.classes_, index=kaggle_test.index), threshold=1)\n",
    "submit_to_kaggle(rdf_more_guesses, 'first try with more than one guess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "### With just more than one guess, the score is immediately shot up to 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning:\n",
      "\n",
      "Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  12 | elapsed:  2.0min remaining:  1.4min\n",
      "[Parallel(n_jobs=10)]: Done  12 out of  12 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=10, penalty='l2', random_state=None,\n",
       "          solver='lbfgs', tol=0.0001, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Try baseline logistic Regression\n",
    "logr = LogisticRegression(verbose=2, n_jobs=10, solver='lbfgs', class_weight='balanced')\n",
    "logr.fit(Xs_train, ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38288203878946875"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr.score(Xs_test, ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 931k/931k [00:07<00:00, 130kB/s]\n"
     ]
    }
   ],
   "source": [
    "submit_to_kaggle(logr.predict(kaggle_test_std), 'first_try for logistic regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4.55M/4.55M [00:05<00:00, 822kB/s]\n"
     ]
    }
   ],
   "source": [
    "### Results form logistic regression seems to be quite terrible on both counts, 0.31 on kggle\n",
    "### Try with more than 1 guesses for each user\n",
    "logr_more_guesses = find_top_choices(pd.DataFrame(logr.predict_proba(kaggle_test_std), columns=logr.classes_, index=kaggle_test_std.index), threshold=1)\n",
    "submit_to_kaggle(logr_more_guesses, filename='FTlogrG', message='first try for logistic_regression with more than one guess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Logistic Regression seems to be inferior in every way with a score of 0.47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning:\n",
      "\n",
      "Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=182, units=48, kernel_initializer=\"uniform\")`\n",
      "\n",
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning:\n",
      "\n",
      "Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=24, kernel_initializer=\"uniform\")`\n",
      "\n",
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning:\n",
      "\n",
      "Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=12, kernel_initializer=\"uniform\")`\n",
      "\n",
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning:\n",
      "\n",
      "The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 16s - loss: 1.1371 - categorical_accuracy: 0.6045\n",
      "Epoch 2/10\n",
      " - 14s - loss: 1.0912 - categorical_accuracy: 0.6160\n",
      "Epoch 3/10\n",
      " - 15s - loss: 1.0805 - categorical_accuracy: 0.6191\n",
      "Epoch 4/10\n",
      " - 14s - loss: 1.0664 - categorical_accuracy: 0.6278\n",
      "Epoch 5/10\n",
      " - 14s - loss: 1.0582 - categorical_accuracy: 0.6338\n",
      "Epoch 6/10\n",
      " - 15s - loss: 1.0540 - categorical_accuracy: 0.6366\n",
      "Epoch 7/10\n",
      " - 15s - loss: 1.0514 - categorical_accuracy: 0.6380\n",
      "Epoch 8/10\n",
      " - 14s - loss: 1.0491 - categorical_accuracy: 0.6396\n",
      "Epoch 9/10\n",
      " - 14s - loss: 1.0476 - categorical_accuracy: 0.6405\n",
      "Epoch 10/10\n",
      " - 15s - loss: 1.0459 - categorical_accuracy: 0.6411\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Dense(output_dim = 48, init = 'uniform', activation = 'relu', input_dim = Xs_train.shape[1]))\n",
    "classifier.add(Dense(output_dim = 24, init = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 12, init = 'uniform', activation = 'sigmoid'))\n",
    "classifier.compile(optimizer = 'adam', loss= 'categorical_crossentropy', metrics = [keras.metrics.categorical_accuracy])\n",
    "classifier.fit(Xs_train, yNN_train, nb_epoch = 10, batch_size= 64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 954k/954k [00:10<00:00, 93.5kB/s]\n"
     ]
    }
   ],
   "source": [
    "submit_to_kaggle(pd.DataFrame([decoder_yNN_train[val] for val in classifier.predict_classes(kaggle_test_std)], columns=['country'], index=kaggle_test_std.index), 'ANN result', 'First try with basic ANN model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2.00M/2.00M [00:07<00:00, 288kB/s]\n"
     ]
    }
   ],
   "source": [
    "### Seems not too bad, comparable with random forest, with a kaggle score of 0.7\n",
    "### Try submitting more guesses for each user\n",
    "ANN_output_df = pd.DataFrame(classifier.predict_proba(kaggle_test_std), index=kaggle_test_std.index).rename(mapper=decoder_yNN_train, axis=1)\n",
    "ANN_more_guesses = find_best_choices(ANN_output_df, threshold=1)\n",
    "submit_to_kaggle(ANN_more_guesses, 'result ANN v2', 'ANN with more guesses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning:\n",
      "\n",
      "Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=182, units=48, kernel_initializer=\"uniform\")`\n",
      "\n",
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning:\n",
      "\n",
      "Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=24, kernel_initializer=\"uniform\")`\n",
      "\n",
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning:\n",
      "\n",
      "Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=12, kernel_initializer=\"uniform\")`\n",
      "\n",
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning:\n",
      "\n",
      "The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 10s - loss: 1.1213 - categorical_accuracy: 0.6080\n",
      "Epoch 2/50\n",
      " - 9s - loss: 1.0828 - categorical_accuracy: 0.6183\n",
      "Epoch 3/50\n",
      " - 9s - loss: 1.0671 - categorical_accuracy: 0.6287\n",
      "Epoch 4/50\n",
      " - 9s - loss: 1.0613 - categorical_accuracy: 0.6320\n",
      "Epoch 5/50\n",
      " - 8s - loss: 1.0573 - categorical_accuracy: 0.6341\n",
      "Epoch 6/50\n",
      " - 8s - loss: 1.0545 - categorical_accuracy: 0.6360\n",
      "Epoch 7/50\n",
      " - 9s - loss: 1.0522 - categorical_accuracy: 0.6373\n",
      "Epoch 8/50\n",
      " - 8s - loss: 1.0510 - categorical_accuracy: 0.6386\n",
      "Epoch 9/50\n",
      " - 9s - loss: 1.0495 - categorical_accuracy: 0.6387\n",
      "Epoch 10/50\n",
      " - 10s - loss: 1.0480 - categorical_accuracy: 0.6403\n",
      "Epoch 11/50\n",
      " - 9s - loss: 1.0467 - categorical_accuracy: 0.6407\n",
      "Epoch 12/50\n",
      " - 8s - loss: 1.0455 - categorical_accuracy: 0.6413\n",
      "Epoch 13/50\n",
      " - 8s - loss: 1.0451 - categorical_accuracy: 0.6410\n",
      "Epoch 14/50\n",
      " - 9s - loss: 1.0439 - categorical_accuracy: 0.6418\n",
      "Epoch 15/50\n",
      " - 9s - loss: 1.0434 - categorical_accuracy: 0.6421\n",
      "Epoch 16/50\n",
      " - 8s - loss: 1.0424 - categorical_accuracy: 0.6423\n",
      "Epoch 17/50\n",
      " - 9s - loss: 1.0416 - categorical_accuracy: 0.6431\n",
      "Epoch 18/50\n",
      " - 9s - loss: 1.0414 - categorical_accuracy: 0.6426\n",
      "Epoch 19/50\n",
      " - 8s - loss: 1.0411 - categorical_accuracy: 0.6434\n",
      "Epoch 20/50\n",
      " - 9s - loss: 1.0403 - categorical_accuracy: 0.6436\n",
      "Epoch 21/50\n",
      " - 9s - loss: 1.0398 - categorical_accuracy: 0.6436\n",
      "Epoch 22/50\n",
      " - 8s - loss: 1.0391 - categorical_accuracy: 0.6444\n",
      "Epoch 23/50\n",
      " - 8s - loss: 1.0387 - categorical_accuracy: 0.6441\n",
      "Epoch 24/50\n",
      " - 9s - loss: 1.0382 - categorical_accuracy: 0.6447\n",
      "Epoch 25/50\n",
      " - 8s - loss: 1.0379 - categorical_accuracy: 0.6447\n",
      "Epoch 26/50\n",
      " - 9s - loss: 1.0373 - categorical_accuracy: 0.6446\n",
      "Epoch 27/50\n",
      " - 9s - loss: 1.0370 - categorical_accuracy: 0.6448\n",
      "Epoch 28/50\n",
      " - 8s - loss: 1.0366 - categorical_accuracy: 0.6446\n",
      "Epoch 29/50\n",
      " - 9s - loss: 1.0361 - categorical_accuracy: 0.6454\n",
      "Epoch 30/50\n",
      " - 8s - loss: 1.0359 - categorical_accuracy: 0.6455\n",
      "Epoch 31/50\n",
      " - 8s - loss: 1.0356 - categorical_accuracy: 0.6449\n",
      "Epoch 32/50\n",
      " - 8s - loss: 1.0356 - categorical_accuracy: 0.6454\n",
      "Epoch 33/50\n",
      " - 9s - loss: 1.0351 - categorical_accuracy: 0.6450\n",
      "Epoch 34/50\n",
      " - 8s - loss: 1.0348 - categorical_accuracy: 0.6452\n",
      "Epoch 35/50\n",
      " - 8s - loss: 1.0346 - categorical_accuracy: 0.6458\n",
      "Epoch 36/50\n",
      " - 8s - loss: 1.0344 - categorical_accuracy: 0.6458\n",
      "Epoch 37/50\n",
      " - 8s - loss: 1.0342 - categorical_accuracy: 0.6460\n",
      "Epoch 38/50\n",
      " - 8s - loss: 1.0338 - categorical_accuracy: 0.6458\n",
      "Epoch 39/50\n",
      " - 9s - loss: 1.0338 - categorical_accuracy: 0.6460\n",
      "Epoch 40/50\n",
      " - 8s - loss: 1.0335 - categorical_accuracy: 0.6461\n",
      "Epoch 41/50\n",
      " - 8s - loss: 1.0331 - categorical_accuracy: 0.6462\n",
      "Epoch 42/50\n",
      " - 8s - loss: 1.0329 - categorical_accuracy: 0.6459\n",
      "Epoch 43/50\n",
      " - 8s - loss: 1.0324 - categorical_accuracy: 0.6466\n",
      "Epoch 44/50\n",
      " - 8s - loss: 1.0327 - categorical_accuracy: 0.6467\n",
      "Epoch 45/50\n",
      " - 9s - loss: 1.0324 - categorical_accuracy: 0.6467\n",
      "Epoch 46/50\n",
      " - 8s - loss: 1.0320 - categorical_accuracy: 0.6471\n",
      "Epoch 47/50\n",
      " - 8s - loss: 1.0324 - categorical_accuracy: 0.6472\n",
      "Epoch 48/50\n",
      " - 9s - loss: 1.0320 - categorical_accuracy: 0.6472\n",
      "Epoch 49/50\n",
      " - 9s - loss: 1.0319 - categorical_accuracy: 0.6473\n",
      "Epoch 50/50\n",
      " - 9s - loss: 1.0316 - categorical_accuracy: 0.6471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x204f71a0438>"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ANN seems to yield similar results as compared to random forest, with a kaggle score of 0.837\n",
    "### Try running ANN with class_weight = 'balanced'\n",
    "classifier1 = Sequential()\n",
    "classifier1.add(Dense(output_dim = 48, init = 'uniform', activation = 'relu', input_dim = Xs.shape[1]))\n",
    "classifier1.add(Dense(output_dim = 24, init = 'uniform', activation = 'relu'))\n",
    "classifier1.add(Dense(output_dim = 12, init = 'uniform', activation = 'sigmoid'))\n",
    "classifier1.compile(optimizer = 'adam', loss= 'categorical_crossentropy', metrics = [keras.metrics.categorical_accuracy])\n",
    "classifier1.fit(Xs, yNN, nb_epoch = 50, batch_size= 32, class_weight='balanced', verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 952k/952k [00:08<00:00, 115kB/s]\n"
     ]
    }
   ],
   "source": [
    "submit_to_kaggle(pd.DataFrame([decoder_yNN[val] for val in classifier1.predict_classes(kaggle_test_std)], columns=['country'], index=kaggle_test_std.index), 'ANN result', 'Re-Trying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1.92M/1.92M [00:08<00:00, 233kB/s]\n"
     ]
    }
   ],
   "source": [
    "### Score is actually noticeably worse with a score of 0.66\n",
    "### Try submitting more guesses for each user\n",
    "ANN_output_df = pd.DataFrame(classifier1.predict_proba(kaggle_test_std), index=kaggle_test_std.index).rename(mapper=decoder_yNN, axis=1)\n",
    "ANN_more_guesses = find_best_choices(ANN_output_df, threshold=1)\n",
    "submit_to_kaggle(ANN_more_guesses, 'ANN result', 'Re-Trying with more guesses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Weaker score than base case ANN but still around the ballpark of 0.833"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-495-e25fc565d22a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mxgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    698\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 700\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m-> 1045\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1046\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "### Try using XGBoost to see how it fair\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ANN_NDF</th>\n",
       "      <th>ANN_US</th>\n",
       "      <th>ANN_other</th>\n",
       "      <th>ANN_FR</th>\n",
       "      <th>ANN_CA</th>\n",
       "      <th>ANN_IT</th>\n",
       "      <th>ANN_ES</th>\n",
       "      <th>ANN_NL</th>\n",
       "      <th>ANN_AU</th>\n",
       "      <th>ANN_GB</th>\n",
       "      <th>...</th>\n",
       "      <th>rdf_DE</th>\n",
       "      <th>rdf_ES</th>\n",
       "      <th>rdf_FR</th>\n",
       "      <th>rdf_GB</th>\n",
       "      <th>rdf_IT</th>\n",
       "      <th>rdf_NDF</th>\n",
       "      <th>rdf_NL</th>\n",
       "      <th>rdf_PT</th>\n",
       "      <th>rdf_US</th>\n",
       "      <th>rdf_other</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34ts65egtp</th>\n",
       "      <td>0.700367</td>\n",
       "      <td>0.009718</td>\n",
       "      <td>0.394277</td>\n",
       "      <td>0.072994</td>\n",
       "      <td>0.011688</td>\n",
       "      <td>0.010483</td>\n",
       "      <td>0.008955</td>\n",
       "      <td>0.025282</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344130</td>\n",
       "      <td>0.190942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128897</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3lh53hchaa</th>\n",
       "      <td>0.763061</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>0.410630</td>\n",
       "      <td>0.055737</td>\n",
       "      <td>0.015304</td>\n",
       "      <td>0.012778</td>\n",
       "      <td>0.015773</td>\n",
       "      <td>0.032121</td>\n",
       "      <td>0.005732</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.454055</td>\n",
       "      <td>0.062359</td>\n",
       "      <td>0.145566</td>\n",
       "      <td>0.147438</td>\n",
       "      <td>0.092988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052957</td>\n",
       "      <td>0.044637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usgaauks8s</th>\n",
       "      <td>0.666912</td>\n",
       "      <td>0.016737</td>\n",
       "      <td>0.779943</td>\n",
       "      <td>0.129980</td>\n",
       "      <td>0.043247</td>\n",
       "      <td>0.025558</td>\n",
       "      <td>0.032278</td>\n",
       "      <td>0.056901</td>\n",
       "      <td>0.016154</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.761898</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dpyusbsbmh</th>\n",
       "      <td>0.813137</td>\n",
       "      <td>0.012636</td>\n",
       "      <td>0.536632</td>\n",
       "      <td>0.102354</td>\n",
       "      <td>0.023468</td>\n",
       "      <td>0.014168</td>\n",
       "      <td>0.025939</td>\n",
       "      <td>0.057246</td>\n",
       "      <td>0.010826</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da5yxikm9j</th>\n",
       "      <td>0.250484</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.811878</td>\n",
       "      <td>0.045154</td>\n",
       "      <td>0.014335</td>\n",
       "      <td>0.050308</td>\n",
       "      <td>0.036908</td>\n",
       "      <td>0.090576</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n4mkqm8tlo</th>\n",
       "      <td>0.573228</td>\n",
       "      <td>0.013512</td>\n",
       "      <td>0.601929</td>\n",
       "      <td>0.099446</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.017646</td>\n",
       "      <td>0.030286</td>\n",
       "      <td>0.045368</td>\n",
       "      <td>0.015435</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e5v3s4xiwe</th>\n",
       "      <td>0.177084</td>\n",
       "      <td>0.016228</td>\n",
       "      <td>0.819414</td>\n",
       "      <td>0.095360</td>\n",
       "      <td>0.038839</td>\n",
       "      <td>0.031417</td>\n",
       "      <td>0.045969</td>\n",
       "      <td>0.085237</td>\n",
       "      <td>0.016718</td>\n",
       "      <td>0.005419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481915</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nifnvygaf0</th>\n",
       "      <td>0.794585</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.511672</td>\n",
       "      <td>0.069489</td>\n",
       "      <td>0.017988</td>\n",
       "      <td>0.012532</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>0.036105</td>\n",
       "      <td>0.009921</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145649</td>\n",
       "      <td>0.209579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kpy5q5en4m</th>\n",
       "      <td>0.420002</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>0.325463</td>\n",
       "      <td>0.018059</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076102</td>\n",
       "      <td>0.790973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074922</td>\n",
       "      <td>0.058003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>js4utkulva</th>\n",
       "      <td>0.800783</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.373938</td>\n",
       "      <td>0.039873</td>\n",
       "      <td>0.014140</td>\n",
       "      <td>0.013144</td>\n",
       "      <td>0.018935</td>\n",
       "      <td>0.040409</td>\n",
       "      <td>0.005239</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195278</td>\n",
       "      <td>0.079629</td>\n",
       "      <td>0.140220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091873</td>\n",
       "      <td>0.128798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a1rv9v6hal</th>\n",
       "      <td>0.777716</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>0.332440</td>\n",
       "      <td>0.065292</td>\n",
       "      <td>0.012158</td>\n",
       "      <td>0.010044</td>\n",
       "      <td>0.014275</td>\n",
       "      <td>0.030709</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yg2zq2yehv</th>\n",
       "      <td>0.921158</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.031861</td>\n",
       "      <td>0.008647</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.008592</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8ude4bv9zb</th>\n",
       "      <td>0.874991</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.150257</td>\n",
       "      <td>0.021361</td>\n",
       "      <td>0.004780</td>\n",
       "      <td>0.005627</td>\n",
       "      <td>0.007935</td>\n",
       "      <td>0.013827</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121847</td>\n",
       "      <td>0.085794</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.099253</td>\n",
       "      <td>0.233568</td>\n",
       "      <td>0.090858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069703</td>\n",
       "      <td>0.055365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8g0kphk925</th>\n",
       "      <td>0.853947</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.305283</td>\n",
       "      <td>0.048790</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>0.011611</td>\n",
       "      <td>0.016593</td>\n",
       "      <td>0.034752</td>\n",
       "      <td>0.004459</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xeh318uqei</th>\n",
       "      <td>0.645940</td>\n",
       "      <td>0.012356</td>\n",
       "      <td>0.732693</td>\n",
       "      <td>0.100807</td>\n",
       "      <td>0.040315</td>\n",
       "      <td>0.019731</td>\n",
       "      <td>0.029959</td>\n",
       "      <td>0.053414</td>\n",
       "      <td>0.017422</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.633114</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kfqogsfolh</th>\n",
       "      <td>0.990626</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.057497</td>\n",
       "      <td>0.008412</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nafvl40z52</th>\n",
       "      <td>0.704547</td>\n",
       "      <td>0.009059</td>\n",
       "      <td>0.428563</td>\n",
       "      <td>0.074635</td>\n",
       "      <td>0.019711</td>\n",
       "      <td>0.011135</td>\n",
       "      <td>0.020019</td>\n",
       "      <td>0.039812</td>\n",
       "      <td>0.007447</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133851</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w92u5mknvh</th>\n",
       "      <td>0.714917</td>\n",
       "      <td>0.009558</td>\n",
       "      <td>0.525400</td>\n",
       "      <td>0.059474</td>\n",
       "      <td>0.017622</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.019217</td>\n",
       "      <td>0.045598</td>\n",
       "      <td>0.008974</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68l0gcw69b</th>\n",
       "      <td>0.354029</td>\n",
       "      <td>0.010748</td>\n",
       "      <td>0.576609</td>\n",
       "      <td>0.104613</td>\n",
       "      <td>0.017014</td>\n",
       "      <td>0.015767</td>\n",
       "      <td>0.022948</td>\n",
       "      <td>0.040335</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actusxyrb5</th>\n",
       "      <td>0.762340</td>\n",
       "      <td>0.007522</td>\n",
       "      <td>0.427852</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>0.019949</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>0.017979</td>\n",
       "      <td>0.038737</td>\n",
       "      <td>0.010496</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938953</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl4rh42ywn</th>\n",
       "      <td>0.738260</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.828942</td>\n",
       "      <td>0.101371</td>\n",
       "      <td>0.005564</td>\n",
       "      <td>0.012876</td>\n",
       "      <td>0.019812</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bc1qld0x5j</th>\n",
       "      <td>0.799360</td>\n",
       "      <td>0.006726</td>\n",
       "      <td>0.273086</td>\n",
       "      <td>0.032583</td>\n",
       "      <td>0.013659</td>\n",
       "      <td>0.013610</td>\n",
       "      <td>0.018872</td>\n",
       "      <td>0.034208</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182804</td>\n",
       "      <td>0.687753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ck6d6gmmxf</th>\n",
       "      <td>0.558171</td>\n",
       "      <td>0.008443</td>\n",
       "      <td>0.496289</td>\n",
       "      <td>0.055162</td>\n",
       "      <td>0.014323</td>\n",
       "      <td>0.015398</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.050272</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151679</td>\n",
       "      <td>0.123786</td>\n",
       "      <td>0.342347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172360</td>\n",
       "      <td>0.076012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g54k0ubth8</th>\n",
       "      <td>0.372097</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>0.652118</td>\n",
       "      <td>0.089772</td>\n",
       "      <td>0.019032</td>\n",
       "      <td>0.016055</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>0.050084</td>\n",
       "      <td>0.011489</td>\n",
       "      <td>0.004875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117442</td>\n",
       "      <td>0.037206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133708</td>\n",
       "      <td>0.223908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4az4ang9ch</th>\n",
       "      <td>0.506160</td>\n",
       "      <td>0.013498</td>\n",
       "      <td>0.565018</td>\n",
       "      <td>0.087595</td>\n",
       "      <td>0.019605</td>\n",
       "      <td>0.031095</td>\n",
       "      <td>0.034129</td>\n",
       "      <td>0.054526</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>0.005939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080254</td>\n",
       "      <td>0.835229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x3xkldy9ab</th>\n",
       "      <td>0.309068</td>\n",
       "      <td>0.009367</td>\n",
       "      <td>0.378198</td>\n",
       "      <td>0.078852</td>\n",
       "      <td>0.012966</td>\n",
       "      <td>0.007853</td>\n",
       "      <td>0.011271</td>\n",
       "      <td>0.026623</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qz1nvzovfd</th>\n",
       "      <td>0.265493</td>\n",
       "      <td>0.011384</td>\n",
       "      <td>0.643512</td>\n",
       "      <td>0.107078</td>\n",
       "      <td>0.017845</td>\n",
       "      <td>0.015464</td>\n",
       "      <td>0.023131</td>\n",
       "      <td>0.038198</td>\n",
       "      <td>0.006828</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9cfp84aber</th>\n",
       "      <td>0.482098</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>0.551303</td>\n",
       "      <td>0.082818</td>\n",
       "      <td>0.015094</td>\n",
       "      <td>0.014185</td>\n",
       "      <td>0.014037</td>\n",
       "      <td>0.050236</td>\n",
       "      <td>0.009376</td>\n",
       "      <td>0.004133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.076050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lngpwy626i</th>\n",
       "      <td>0.365071</td>\n",
       "      <td>0.015075</td>\n",
       "      <td>0.815346</td>\n",
       "      <td>0.096777</td>\n",
       "      <td>0.033621</td>\n",
       "      <td>0.028317</td>\n",
       "      <td>0.054622</td>\n",
       "      <td>0.056565</td>\n",
       "      <td>0.017318</td>\n",
       "      <td>0.004417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.042002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.827998</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7w85wd33tm</th>\n",
       "      <td>0.899322</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.133911</td>\n",
       "      <td>0.053011</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.006287</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5wldmwjbl2</th>\n",
       "      <td>0.875947</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.068454</td>\n",
       "      <td>0.023375</td>\n",
       "      <td>0.003615</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>0.003680</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r0uy1g9otb</th>\n",
       "      <td>0.760324</td>\n",
       "      <td>0.003415</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.025050</td>\n",
       "      <td>0.005669</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>0.006246</td>\n",
       "      <td>0.013907</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042421</td>\n",
       "      <td>0.101241</td>\n",
       "      <td>0.117523</td>\n",
       "      <td>0.121140</td>\n",
       "      <td>0.063903</td>\n",
       "      <td>0.207765</td>\n",
       "      <td>0.043695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083084</td>\n",
       "      <td>0.108083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o018zuweaw</th>\n",
       "      <td>0.792394</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>0.132303</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>0.006257</td>\n",
       "      <td>0.014274</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052748</td>\n",
       "      <td>0.039603</td>\n",
       "      <td>0.111402</td>\n",
       "      <td>0.098866</td>\n",
       "      <td>0.138816</td>\n",
       "      <td>0.173785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105811</td>\n",
       "      <td>0.062059</td>\n",
       "      <td>0.069208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mz8jhec03o</th>\n",
       "      <td>0.654169</td>\n",
       "      <td>0.006223</td>\n",
       "      <td>0.347827</td>\n",
       "      <td>0.080141</td>\n",
       "      <td>0.011945</td>\n",
       "      <td>0.006759</td>\n",
       "      <td>0.015893</td>\n",
       "      <td>0.014768</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7xti52bg09</th>\n",
       "      <td>0.831406</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.105055</td>\n",
       "      <td>0.026362</td>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.005313</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6ica3u2g52</th>\n",
       "      <td>0.560820</td>\n",
       "      <td>0.006169</td>\n",
       "      <td>0.565246</td>\n",
       "      <td>0.071307</td>\n",
       "      <td>0.007720</td>\n",
       "      <td>0.009317</td>\n",
       "      <td>0.011325</td>\n",
       "      <td>0.021622</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.003272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068824</td>\n",
       "      <td>0.360908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cwdxkw41rm</th>\n",
       "      <td>0.941515</td>\n",
       "      <td>0.019845</td>\n",
       "      <td>0.178317</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>0.036131</td>\n",
       "      <td>0.027892</td>\n",
       "      <td>0.055692</td>\n",
       "      <td>0.048118</td>\n",
       "      <td>0.011404</td>\n",
       "      <td>0.006953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211495</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>umcbpopwoq</th>\n",
       "      <td>0.918556</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.138918</td>\n",
       "      <td>0.021881</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.004430</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262733</td>\n",
       "      <td>0.594184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143083</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72f8g1e2qn</th>\n",
       "      <td>0.920099</td>\n",
       "      <td>0.003438</td>\n",
       "      <td>0.094634</td>\n",
       "      <td>0.027391</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hzfwg19rj9</th>\n",
       "      <td>0.327841</td>\n",
       "      <td>0.013170</td>\n",
       "      <td>0.669512</td>\n",
       "      <td>0.112183</td>\n",
       "      <td>0.021408</td>\n",
       "      <td>0.019265</td>\n",
       "      <td>0.024611</td>\n",
       "      <td>0.049703</td>\n",
       "      <td>0.007619</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t43r3hw9h1</th>\n",
       "      <td>0.788370</td>\n",
       "      <td>0.008636</td>\n",
       "      <td>0.357304</td>\n",
       "      <td>0.048498</td>\n",
       "      <td>0.016388</td>\n",
       "      <td>0.014363</td>\n",
       "      <td>0.015182</td>\n",
       "      <td>0.037932</td>\n",
       "      <td>0.008706</td>\n",
       "      <td>0.004185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127679</td>\n",
       "      <td>0.169951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8n1h553d4p</th>\n",
       "      <td>0.777614</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>0.166065</td>\n",
       "      <td>0.020972</td>\n",
       "      <td>0.005163</td>\n",
       "      <td>0.004612</td>\n",
       "      <td>0.006645</td>\n",
       "      <td>0.014781</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044346</td>\n",
       "      <td>0.079307</td>\n",
       "      <td>0.103975</td>\n",
       "      <td>0.124312</td>\n",
       "      <td>0.087611</td>\n",
       "      <td>0.167551</td>\n",
       "      <td>0.043290</td>\n",
       "      <td>0.081657</td>\n",
       "      <td>0.088049</td>\n",
       "      <td>0.064430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z8f3mrhtfi</th>\n",
       "      <td>0.855538</td>\n",
       "      <td>0.009746</td>\n",
       "      <td>0.455411</td>\n",
       "      <td>0.065656</td>\n",
       "      <td>0.019868</td>\n",
       "      <td>0.014037</td>\n",
       "      <td>0.015364</td>\n",
       "      <td>0.031666</td>\n",
       "      <td>0.011195</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150209</td>\n",
       "      <td>0.620252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229539</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caiymsu75v</th>\n",
       "      <td>0.465739</td>\n",
       "      <td>0.010240</td>\n",
       "      <td>0.761651</td>\n",
       "      <td>0.098852</td>\n",
       "      <td>0.020220</td>\n",
       "      <td>0.024856</td>\n",
       "      <td>0.027671</td>\n",
       "      <td>0.043566</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>0.004782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875890</td>\n",
       "      <td>0.044110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gxwcseiz1m</th>\n",
       "      <td>0.924543</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.073782</td>\n",
       "      <td>0.042982</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>0.010077</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.395201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216606</td>\n",
       "      <td>0.388193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fmfd96iv76</th>\n",
       "      <td>0.974275</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.029507</td>\n",
       "      <td>0.006773</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o6ekt4j3f8</th>\n",
       "      <td>0.355558</td>\n",
       "      <td>0.007125</td>\n",
       "      <td>0.367301</td>\n",
       "      <td>0.064409</td>\n",
       "      <td>0.016142</td>\n",
       "      <td>0.009970</td>\n",
       "      <td>0.013898</td>\n",
       "      <td>0.028853</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j47mkts84o</th>\n",
       "      <td>0.815885</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.322761</td>\n",
       "      <td>0.021794</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.006646</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282670</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g4chgzoh5u</th>\n",
       "      <td>0.901685</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.256102</td>\n",
       "      <td>0.038753</td>\n",
       "      <td>0.009273</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71mg196tuz</th>\n",
       "      <td>0.440328</td>\n",
       "      <td>0.013476</td>\n",
       "      <td>0.655262</td>\n",
       "      <td>0.091729</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.016647</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>0.050185</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>0.004276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256855</td>\n",
       "      <td>0.113507</td>\n",
       "      <td>0.068062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088578</td>\n",
       "      <td>0.060671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163271</td>\n",
       "      <td>0.067910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8gnmz85gcr</th>\n",
       "      <td>0.834203</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.471652</td>\n",
       "      <td>0.057511</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>0.015671</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>0.034888</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.003370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105945</td>\n",
       "      <td>0.257356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3zggv5popf</th>\n",
       "      <td>0.825748</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>0.193182</td>\n",
       "      <td>0.025429</td>\n",
       "      <td>0.006847</td>\n",
       "      <td>0.006687</td>\n",
       "      <td>0.007713</td>\n",
       "      <td>0.017445</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070118</td>\n",
       "      <td>0.100476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175974</td>\n",
       "      <td>0.348568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073343</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6x2eco329m</th>\n",
       "      <td>0.560405</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>0.329181</td>\n",
       "      <td>0.041909</td>\n",
       "      <td>0.006784</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>0.010311</td>\n",
       "      <td>0.019261</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.505523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494477</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbyi78kubx</th>\n",
       "      <td>0.416027</td>\n",
       "      <td>0.012812</td>\n",
       "      <td>0.702431</td>\n",
       "      <td>0.084128</td>\n",
       "      <td>0.027294</td>\n",
       "      <td>0.024417</td>\n",
       "      <td>0.030118</td>\n",
       "      <td>0.073066</td>\n",
       "      <td>0.011418</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128828</td>\n",
       "      <td>0.070850</td>\n",
       "      <td>0.120265</td>\n",
       "      <td>0.112221</td>\n",
       "      <td>0.019327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367864</td>\n",
       "      <td>0.100211</td>\n",
       "      <td>0.080433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nxmjjn2jli</th>\n",
       "      <td>0.407966</td>\n",
       "      <td>0.019226</td>\n",
       "      <td>0.783918</td>\n",
       "      <td>0.127975</td>\n",
       "      <td>0.041485</td>\n",
       "      <td>0.042213</td>\n",
       "      <td>0.051281</td>\n",
       "      <td>0.072813</td>\n",
       "      <td>0.014578</td>\n",
       "      <td>0.008871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d32a4r838u</th>\n",
       "      <td>0.858760</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.521773</td>\n",
       "      <td>0.065008</td>\n",
       "      <td>0.020638</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.041134</td>\n",
       "      <td>0.009383</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232432</td>\n",
       "      <td>0.200177</td>\n",
       "      <td>0.195389</td>\n",
       "      <td>0.201698</td>\n",
       "      <td>0.079485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037140</td>\n",
       "      <td>0.053678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l4zsyi7tio</th>\n",
       "      <td>0.747141</td>\n",
       "      <td>0.004534</td>\n",
       "      <td>0.279003</td>\n",
       "      <td>0.027160</td>\n",
       "      <td>0.006225</td>\n",
       "      <td>0.007927</td>\n",
       "      <td>0.012679</td>\n",
       "      <td>0.024728</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017015</td>\n",
       "      <td>0.139299</td>\n",
       "      <td>0.062085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134791</td>\n",
       "      <td>0.198508</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325815</td>\n",
       "      <td>0.092487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jnve2rpge1</th>\n",
       "      <td>0.853708</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>0.173801</td>\n",
       "      <td>0.042872</td>\n",
       "      <td>0.009449</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>0.027451</td>\n",
       "      <td>0.023631</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vjh9dqfmvu</th>\n",
       "      <td>0.821054</td>\n",
       "      <td>0.009783</td>\n",
       "      <td>0.490461</td>\n",
       "      <td>0.064245</td>\n",
       "      <td>0.019221</td>\n",
       "      <td>0.014741</td>\n",
       "      <td>0.018608</td>\n",
       "      <td>0.039389</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391374</td>\n",
       "      <td>0.079922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093847</td>\n",
       "      <td>0.196022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g372oxguj2</th>\n",
       "      <td>0.916208</td>\n",
       "      <td>0.002230</td>\n",
       "      <td>0.245296</td>\n",
       "      <td>0.028315</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.011171</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192105 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ANN_NDF    ANN_US  ANN_other    ANN_FR    ANN_CA    ANN_IT  \\\n",
       "id                                                                        \n",
       "34ts65egtp  0.700367  0.009718   0.394277  0.072994  0.011688  0.010483   \n",
       "3lh53hchaa  0.763061  0.008084   0.410630  0.055737  0.015304  0.012778   \n",
       "usgaauks8s  0.666912  0.016737   0.779943  0.129980  0.043247  0.025558   \n",
       "dpyusbsbmh  0.813137  0.012636   0.536632  0.102354  0.023468  0.014168   \n",
       "da5yxikm9j  0.250484  0.004193   0.811878  0.045154  0.014335  0.050308   \n",
       "n4mkqm8tlo  0.573228  0.013512   0.601929  0.099446  0.027500  0.017646   \n",
       "e5v3s4xiwe  0.177084  0.016228   0.819414  0.095360  0.038839  0.031417   \n",
       "nifnvygaf0  0.794585  0.010554   0.511672  0.069489  0.017988  0.012532   \n",
       "kpy5q5en4m  0.420002  0.001758   0.325463  0.018059  0.001459  0.002432   \n",
       "js4utkulva  0.800783  0.005988   0.373938  0.039873  0.014140  0.013144   \n",
       "a1rv9v6hal  0.777716  0.006771   0.332440  0.065292  0.012158  0.010044   \n",
       "yg2zq2yehv  0.921158  0.000877   0.031861  0.008647  0.003955  0.001793   \n",
       "8ude4bv9zb  0.874991  0.002950   0.150257  0.021361  0.004780  0.005627   \n",
       "8g0kphk925  0.853947  0.006127   0.305283  0.048790  0.011996  0.011611   \n",
       "xeh318uqei  0.645940  0.012356   0.732693  0.100807  0.040315  0.019731   \n",
       "kfqogsfolh  0.990626  0.001507   0.057497  0.008412  0.001021  0.000576   \n",
       "nafvl40z52  0.704547  0.009059   0.428563  0.074635  0.019711  0.011135   \n",
       "w92u5mknvh  0.714917  0.009558   0.525400  0.059474  0.017622  0.015152   \n",
       "68l0gcw69b  0.354029  0.010748   0.576609  0.104613  0.017014  0.015767   \n",
       "actusxyrb5  0.762340  0.007522   0.427852  0.055587  0.019949  0.011884   \n",
       "tl4rh42ywn  0.738260  0.003017   0.828942  0.101371  0.005564  0.012876   \n",
       "bc1qld0x5j  0.799360  0.006726   0.273086  0.032583  0.013659  0.013610   \n",
       "ck6d6gmmxf  0.558171  0.008443   0.496289  0.055162  0.014323  0.015398   \n",
       "g54k0ubth8  0.372097  0.014118   0.652118  0.089772  0.019032  0.016055   \n",
       "4az4ang9ch  0.506160  0.013498   0.565018  0.087595  0.019605  0.031095   \n",
       "x3xkldy9ab  0.309068  0.009367   0.378198  0.078852  0.012966  0.007853   \n",
       "qz1nvzovfd  0.265493  0.011384   0.643512  0.107078  0.017845  0.015464   \n",
       "9cfp84aber  0.482098  0.012171   0.551303  0.082818  0.015094  0.014185   \n",
       "lngpwy626i  0.365071  0.015075   0.815346  0.096777  0.033621  0.028317   \n",
       "7w85wd33tm  0.899322  0.000668   0.133911  0.053011  0.001756  0.000984   \n",
       "...              ...       ...        ...       ...       ...       ...   \n",
       "5wldmwjbl2  0.875947  0.002034   0.068454  0.023375  0.003615  0.002471   \n",
       "r0uy1g9otb  0.760324  0.003415   0.135120  0.025050  0.005669  0.005173   \n",
       "o018zuweaw  0.792394  0.002971   0.132303  0.022068  0.004657  0.003996   \n",
       "mz8jhec03o  0.654169  0.006223   0.347827  0.080141  0.011945  0.006759   \n",
       "7xti52bg09  0.831406  0.002518   0.105055  0.026362  0.005119  0.003940   \n",
       "6ica3u2g52  0.560820  0.006169   0.565246  0.071307  0.007720  0.009317   \n",
       "cwdxkw41rm  0.941515  0.019845   0.178317  0.044249  0.036131  0.027892   \n",
       "umcbpopwoq  0.918556  0.001404   0.138918  0.021881  0.001747  0.001293   \n",
       "72f8g1e2qn  0.920099  0.003438   0.094634  0.027391  0.005579  0.003978   \n",
       "hzfwg19rj9  0.327841  0.013170   0.669512  0.112183  0.021408  0.019265   \n",
       "t43r3hw9h1  0.788370  0.008636   0.357304  0.048498  0.016388  0.014363   \n",
       "8n1h553d4p  0.777614  0.003225   0.166065  0.020972  0.005163  0.004612   \n",
       "z8f3mrhtfi  0.855538  0.009746   0.455411  0.065656  0.019868  0.014037   \n",
       "caiymsu75v  0.465739  0.010240   0.761651  0.098852  0.020220  0.024856   \n",
       "gxwcseiz1m  0.924543  0.001398   0.073782  0.042982  0.002801  0.001885   \n",
       "fmfd96iv76  0.974275  0.000665   0.029507  0.006773  0.001228  0.000561   \n",
       "o6ekt4j3f8  0.355558  0.007125   0.367301  0.064409  0.016142  0.009970   \n",
       "j47mkts84o  0.815885  0.001770   0.322761  0.021794  0.003207  0.003673   \n",
       "g4chgzoh5u  0.901685  0.003048   0.256102  0.038753  0.009273  0.004724   \n",
       "71mg196tuz  0.440328  0.013476   0.655262  0.091729  0.021100  0.016647   \n",
       "8gnmz85gcr  0.834203  0.008165   0.471652  0.057511  0.019907  0.015671   \n",
       "3zggv5popf  0.825748  0.003331   0.193182  0.025429  0.006847  0.006687   \n",
       "6x2eco329m  0.560405  0.004855   0.329181  0.041909  0.006784  0.007369   \n",
       "gbyi78kubx  0.416027  0.012812   0.702431  0.084128  0.027294  0.024417   \n",
       "nxmjjn2jli  0.407966  0.019226   0.783918  0.127975  0.041485  0.042213   \n",
       "d32a4r838u  0.858760  0.008700   0.521773  0.065008  0.020638  0.015000   \n",
       "l4zsyi7tio  0.747141  0.004534   0.279003  0.027160  0.006225  0.007927   \n",
       "jnve2rpge1  0.853708  0.004425   0.173801  0.042872  0.009449  0.009237   \n",
       "vjh9dqfmvu  0.821054  0.009783   0.490461  0.064245  0.019221  0.014741   \n",
       "g372oxguj2  0.916208  0.002230   0.245296  0.028315  0.007183  0.003955   \n",
       "\n",
       "              ANN_ES    ANN_NL    ANN_AU    ANN_GB    ...        rdf_DE  \\\n",
       "id                                                    ...                 \n",
       "34ts65egtp  0.008955  0.025282  0.004535  0.002785    ...      0.336032   \n",
       "3lh53hchaa  0.015773  0.032121  0.005732  0.003019    ...      0.000000   \n",
       "usgaauks8s  0.032278  0.056901  0.016154  0.003459    ...      0.010000   \n",
       "dpyusbsbmh  0.025939  0.057246  0.010826  0.004402    ...      0.000000   \n",
       "da5yxikm9j  0.036908  0.090576  0.003435  0.001632    ...      0.000000   \n",
       "n4mkqm8tlo  0.030286  0.045368  0.015435  0.004336    ...      0.000000   \n",
       "e5v3s4xiwe  0.045969  0.085237  0.016718  0.005419    ...      0.000000   \n",
       "nifnvygaf0  0.011244  0.036105  0.009921  0.003312    ...      0.000000   \n",
       "kpy5q5en4m  0.001456  0.005423  0.000655  0.001835    ...      0.000000   \n",
       "js4utkulva  0.018935  0.040409  0.005239  0.002989    ...      0.000000   \n",
       "a1rv9v6hal  0.014275  0.030709  0.003840  0.002424    ...      0.000000   \n",
       "yg2zq2yehv  0.002414  0.008592  0.000384  0.000556    ...      0.010000   \n",
       "8ude4bv9zb  0.007935  0.013827  0.001209  0.001243    ...      0.000000   \n",
       "8g0kphk925  0.016593  0.034752  0.004459  0.002381    ...      0.000000   \n",
       "xeh318uqei  0.029959  0.053414  0.017422  0.002559    ...      0.000000   \n",
       "kfqogsfolh  0.001247  0.001157  0.000230  0.000168    ...      0.000000   \n",
       "nafvl40z52  0.020019  0.039812  0.007447  0.003359    ...      0.000000   \n",
       "w92u5mknvh  0.019217  0.045598  0.008974  0.003486    ...      0.000000   \n",
       "68l0gcw69b  0.022948  0.040335  0.005324  0.002862    ...      0.010000   \n",
       "actusxyrb5  0.017979  0.038737  0.010496  0.003156    ...      0.000000   \n",
       "tl4rh42ywn  0.019812  0.011340  0.001115  0.001040    ...      0.020000   \n",
       "bc1qld0x5j  0.018872  0.034208  0.004701  0.003122    ...      0.000000   \n",
       "ck6d6gmmxf  0.019590  0.050272  0.006115  0.003295    ...      0.000000   \n",
       "g54k0ubth8  0.016302  0.050084  0.011489  0.004875    ...      0.257419   \n",
       "4az4ang9ch  0.034129  0.054526  0.007951  0.005939    ...      0.000000   \n",
       "x3xkldy9ab  0.011271  0.026623  0.002580  0.002796    ...      0.000000   \n",
       "qz1nvzovfd  0.023131  0.038198  0.006828  0.002602    ...      0.000000   \n",
       "9cfp84aber  0.014037  0.050236  0.009376  0.004133    ...      0.000000   \n",
       "lngpwy626i  0.054622  0.056565  0.017318  0.004417    ...      0.000000   \n",
       "7w85wd33tm  0.002681  0.006287  0.000178  0.000214    ...      0.000000   \n",
       "...              ...       ...       ...       ...    ...           ...   \n",
       "5wldmwjbl2  0.003680  0.007407  0.000582  0.000428    ...      0.000000   \n",
       "r0uy1g9otb  0.006246  0.013907  0.001927  0.001092    ...      0.042421   \n",
       "o018zuweaw  0.006257  0.014274  0.001355  0.000885    ...      0.052748   \n",
       "mz8jhec03o  0.015893  0.014768  0.002580  0.001013    ...      0.000000   \n",
       "7xti52bg09  0.005313  0.010500  0.001194  0.000783    ...      0.000000   \n",
       "6ica3u2g52  0.011325  0.021622  0.003155  0.003272    ...      0.000000   \n",
       "cwdxkw41rm  0.055692  0.048118  0.011404  0.006953    ...      0.000000   \n",
       "umcbpopwoq  0.001065  0.004430  0.000393  0.000295    ...      0.000000   \n",
       "72f8g1e2qn  0.007380  0.011686  0.001329  0.001036    ...      0.000000   \n",
       "hzfwg19rj9  0.024611  0.049703  0.007619  0.005038    ...      0.000000   \n",
       "t43r3hw9h1  0.015182  0.037932  0.008706  0.004185    ...      0.382361   \n",
       "8n1h553d4p  0.006645  0.014781  0.002107  0.000841    ...      0.044346   \n",
       "z8f3mrhtfi  0.015364  0.031666  0.011195  0.002981    ...      0.000000   \n",
       "caiymsu75v  0.027671  0.043566  0.006751  0.004782    ...      0.000000   \n",
       "gxwcseiz1m  0.006182  0.010077  0.000358  0.000288    ...      0.000000   \n",
       "fmfd96iv76  0.001198  0.002129  0.000301  0.000119    ...      0.000000   \n",
       "o6ekt4j3f8  0.013898  0.028853  0.004001  0.001955    ...      0.000000   \n",
       "j47mkts84o  0.006646  0.006862  0.000545  0.000651    ...      0.000000   \n",
       "g4chgzoh5u  0.006236  0.012008  0.001537  0.001342    ...      0.000000   \n",
       "71mg196tuz  0.016985  0.050185  0.011803  0.004276    ...      0.256855   \n",
       "8gnmz85gcr  0.016221  0.034888  0.007275  0.003370    ...      0.000000   \n",
       "3zggv5popf  0.007713  0.017445  0.002494  0.001282    ...      0.132484   \n",
       "6x2eco329m  0.010311  0.019261  0.001881  0.001698    ...      0.000000   \n",
       "gbyi78kubx  0.030118  0.073066  0.011418  0.004899    ...      0.000000   \n",
       "nxmjjn2jli  0.051281  0.072813  0.014578  0.008871    ...      0.000000   \n",
       "d32a4r838u  0.019590  0.041134  0.009383  0.002545    ...      0.000000   \n",
       "l4zsyi7tio  0.012679  0.024728  0.002589  0.001378    ...      0.017015   \n",
       "jnve2rpge1  0.027451  0.023631  0.002668  0.001399    ...      0.000000   \n",
       "vjh9dqfmvu  0.018608  0.039389  0.008386  0.003311    ...      0.000000   \n",
       "g372oxguj2  0.005208  0.011171  0.001298  0.001012    ...      0.000000   \n",
       "\n",
       "              rdf_ES    rdf_FR    rdf_GB    rdf_IT   rdf_NDF    rdf_NL  \\\n",
       "id                                                                       \n",
       "34ts65egtp  0.000000  0.000000  0.000000  0.344130  0.190942  0.000000   \n",
       "3lh53hchaa  0.454055  0.062359  0.145566  0.147438  0.092988  0.000000   \n",
       "usgaauks8s  0.000000  0.000000  0.000000  0.000000  0.158102  0.000000   \n",
       "dpyusbsbmh  0.000000  0.000000  0.000000  0.020000  0.160000  0.000000   \n",
       "da5yxikm9j  0.000000  0.010000  0.010000  0.000000  0.150000  0.000000   \n",
       "n4mkqm8tlo  0.000000  0.000000  0.000000  0.000000  0.040000  0.000000   \n",
       "e5v3s4xiwe  0.000000  0.000000  0.000000  0.000000  0.066025  0.000000   \n",
       "nifnvygaf0  0.000000  0.204436  0.000000  0.000000  0.078505  0.000000   \n",
       "kpy5q5en4m  0.000000  0.000000  0.000000  0.076102  0.790973  0.000000   \n",
       "js4utkulva  0.195278  0.079629  0.140220  0.000000  0.082909  0.000000   \n",
       "a1rv9v6hal  0.000000  0.030000  0.000000  0.000000  0.850000  0.000000   \n",
       "yg2zq2yehv  0.020000  0.010000  0.000000  0.000000  0.860000  0.000000   \n",
       "8ude4bv9zb  0.121847  0.085794  0.026846  0.099253  0.233568  0.090858   \n",
       "8g0kphk925  0.010000  0.000000  0.000000  0.000000  0.810000  0.000000   \n",
       "xeh318uqei  0.019326  0.000000  0.000000  0.000000  0.007560  0.000000   \n",
       "kfqogsfolh  0.000000  0.000000  0.000000  0.000000  0.960000  0.000000   \n",
       "nafvl40z52  0.000000  0.660000  0.000000  0.000000  0.206149  0.000000   \n",
       "w92u5mknvh  0.000000  0.010000  0.020000  0.000000  0.530000  0.000000   \n",
       "68l0gcw69b  0.020000  0.010000  0.000000  0.010000  0.650000  0.000000   \n",
       "actusxyrb5  0.000000  0.000000  0.000000  0.000000  0.061047  0.000000   \n",
       "tl4rh42ywn  0.000000  0.000000  0.000000  0.010000  0.070000  0.000000   \n",
       "bc1qld0x5j  0.000000  0.000000  0.000000  0.000000  0.129442  0.000000   \n",
       "ck6d6gmmxf  0.151679  0.123786  0.342347  0.000000  0.133816  0.000000   \n",
       "g54k0ubth8  0.000000  0.230317  0.000000  0.117442  0.037206  0.000000   \n",
       "4az4ang9ch  0.000000  0.000000  0.000000  0.000000  0.084517  0.000000   \n",
       "x3xkldy9ab  0.000000  0.011254  0.000000  0.000000  0.710000  0.000000   \n",
       "qz1nvzovfd  0.010000  0.030000  0.000000  0.020000  0.480000  0.000000   \n",
       "9cfp84aber  0.010000  0.000000  0.000000  0.000000  0.183950  0.000000   \n",
       "lngpwy626i  0.040000  0.000000  0.000000  0.090000  0.042002  0.000000   \n",
       "7w85wd33tm  0.000000  0.000000  0.000000  0.000000  0.330000  0.000000   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "5wldmwjbl2  0.000000  0.000000  0.000000  0.000000  0.970000  0.000000   \n",
       "r0uy1g9otb  0.101241  0.117523  0.121140  0.063903  0.207765  0.043695   \n",
       "o018zuweaw  0.039603  0.111402  0.098866  0.138816  0.173785  0.000000   \n",
       "mz8jhec03o  0.000000  0.010000  0.010000  0.000000  0.850000  0.000000   \n",
       "7xti52bg09  0.010000  0.010000  0.000000  0.000000  0.910000  0.000000   \n",
       "6ica3u2g52  0.000000  0.000000  0.000000  0.000000  0.064012  0.000000   \n",
       "cwdxkw41rm  0.000000  0.000000  0.048505  0.000000  0.060000  0.000000   \n",
       "umcbpopwoq  0.000000  0.000000  0.000000  0.000000  0.262733  0.594184   \n",
       "72f8g1e2qn  0.000000  0.000000  0.000000  0.000000  0.990000  0.000000   \n",
       "hzfwg19rj9  0.000000  0.000000  0.000000  0.010000  0.130000  0.000000   \n",
       "t43r3hw9h1  0.000000  0.197821  0.000000  0.000000  0.122189  0.000000   \n",
       "8n1h553d4p  0.079307  0.103975  0.124312  0.087611  0.167551  0.043290   \n",
       "z8f3mrhtfi  0.000000  0.000000  0.000000  0.000000  0.150209  0.620252   \n",
       "caiymsu75v  0.000000  0.020000  0.000000  0.000000  0.050000  0.000000   \n",
       "gxwcseiz1m  0.000000  0.000000  0.000000  0.000000  0.395201  0.000000   \n",
       "fmfd96iv76  0.000000  0.000000  0.000000  0.000000  0.980000  0.000000   \n",
       "o6ekt4j3f8  0.010000  0.020000  0.040000  0.020000  0.790000  0.000000   \n",
       "j47mkts84o  0.558940  0.000000  0.000000  0.000000  0.158389  0.000000   \n",
       "g4chgzoh5u  0.000000  0.000000  0.000000  0.000000  0.250000  0.000000   \n",
       "71mg196tuz  0.113507  0.068062  0.000000  0.088578  0.060671  0.000000   \n",
       "8gnmz85gcr  0.000000  0.355573  0.000000  0.000000  0.281126  0.000000   \n",
       "3zggv5popf  0.000000  0.070118  0.100476  0.000000  0.175974  0.348568   \n",
       "6x2eco329m  0.000000  0.505523  0.000000  0.000000  0.000000  0.000000   \n",
       "gbyi78kubx  0.128828  0.070850  0.120265  0.112221  0.019327  0.000000   \n",
       "nxmjjn2jli  0.010000  0.000000  0.010000  0.010000  0.130000  0.010000   \n",
       "d32a4r838u  0.232432  0.200177  0.195389  0.201698  0.079485  0.000000   \n",
       "l4zsyi7tio  0.139299  0.062085  0.000000  0.134791  0.198508  0.030000   \n",
       "jnve2rpge1  0.000000  0.000000  0.000000  0.020000  0.840000  0.010000   \n",
       "vjh9dqfmvu  0.098925  0.000000  0.000000  0.391374  0.079922  0.000000   \n",
       "g372oxguj2  0.000000  0.000000  0.000000  0.000000  0.340000  0.000000   \n",
       "\n",
       "              rdf_PT    rdf_US  rdf_other  \n",
       "id                                         \n",
       "34ts65egtp  0.000000  0.128897   0.000000  \n",
       "3lh53hchaa  0.000000  0.052957   0.044637  \n",
       "usgaauks8s  0.000000  0.761898   0.030000  \n",
       "dpyusbsbmh  0.000000  0.730000   0.090000  \n",
       "da5yxikm9j  0.000000  0.800000   0.030000  \n",
       "n4mkqm8tlo  0.000000  0.960000   0.000000  \n",
       "e5v3s4xiwe  0.000000  0.481915   0.000000  \n",
       "nifnvygaf0  0.000000  0.145649   0.209579  \n",
       "kpy5q5en4m  0.000000  0.074922   0.058003  \n",
       "js4utkulva  0.000000  0.091873   0.128798  \n",
       "a1rv9v6hal  0.000000  0.110000   0.000000  \n",
       "yg2zq2yehv  0.000000  0.080000   0.020000  \n",
       "8ude4bv9zb  0.000000  0.069703   0.055365  \n",
       "8g0kphk925  0.000000  0.180000   0.000000  \n",
       "xeh318uqei  0.000000  0.633114   0.000000  \n",
       "kfqogsfolh  0.000000  0.040000   0.000000  \n",
       "nafvl40z52  0.000000  0.133851   0.000000  \n",
       "w92u5mknvh  0.000000  0.410000   0.020000  \n",
       "68l0gcw69b  0.000000  0.260000   0.040000  \n",
       "actusxyrb5  0.000000  0.938953   0.000000  \n",
       "tl4rh42ywn  0.000000  0.870000   0.020000  \n",
       "bc1qld0x5j  0.000000  0.182804   0.687753  \n",
       "ck6d6gmmxf  0.000000  0.172360   0.076012  \n",
       "g54k0ubth8  0.000000  0.133708   0.223908  \n",
       "4az4ang9ch  0.000000  0.080254   0.835229  \n",
       "x3xkldy9ab  0.000000  0.230000   0.040000  \n",
       "qz1nvzovfd  0.000000  0.410000   0.050000  \n",
       "9cfp84aber  0.000000  0.020000   0.076050  \n",
       "lngpwy626i  0.000000  0.827998   0.000000  \n",
       "7w85wd33tm  0.000000  0.670000   0.000000  \n",
       "...              ...       ...        ...  \n",
       "5wldmwjbl2  0.000000  0.030000   0.000000  \n",
       "r0uy1g9otb  0.000000  0.083084   0.108083  \n",
       "o018zuweaw  0.105811  0.062059   0.069208  \n",
       "mz8jhec03o  0.000000  0.090000   0.020000  \n",
       "7xti52bg09  0.000000  0.040000   0.030000  \n",
       "6ica3u2g52  0.000000  0.068824   0.360908  \n",
       "cwdxkw41rm  0.000000  0.211495   0.010000  \n",
       "umcbpopwoq  0.000000  0.143083   0.000000  \n",
       "72f8g1e2qn  0.000000  0.010000   0.000000  \n",
       "hzfwg19rj9  0.000000  0.840000   0.020000  \n",
       "t43r3hw9h1  0.000000  0.127679   0.169951  \n",
       "8n1h553d4p  0.081657  0.088049   0.064430  \n",
       "z8f3mrhtfi  0.000000  0.229539   0.000000  \n",
       "caiymsu75v  0.000000  0.875890   0.044110  \n",
       "gxwcseiz1m  0.000000  0.216606   0.388193  \n",
       "fmfd96iv76  0.000000  0.020000   0.000000  \n",
       "o6ekt4j3f8  0.000000  0.080000   0.040000  \n",
       "j47mkts84o  0.000000  0.282670   0.000000  \n",
       "g4chgzoh5u  0.000000  0.720000   0.020000  \n",
       "71mg196tuz  0.000000  0.163271   0.067910  \n",
       "8gnmz85gcr  0.000000  0.105945   0.257356  \n",
       "3zggv5popf  0.000000  0.073343   0.050900  \n",
       "6x2eco329m  0.000000  0.494477   0.000000  \n",
       "gbyi78kubx  0.367864  0.100211   0.080433  \n",
       "nxmjjn2jli  0.000000  0.210000   0.610000  \n",
       "d32a4r838u  0.000000  0.037140   0.053678  \n",
       "l4zsyi7tio  0.000000  0.325815   0.092487  \n",
       "jnve2rpge1  0.000000  0.100000   0.030000  \n",
       "vjh9dqfmvu  0.000000  0.093847   0.196022  \n",
       "g372oxguj2  0.000000  0.630000   0.030000  \n",
       "\n",
       "[192105 rows x 24 columns]"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Both RandomForest and ANN seem to yield similar results\n",
    "### lets try stacking both models and see if we get a better yield\n",
    "\n",
    "### Create derivative features for ANN and rdf for the TRAIN data\n",
    "\n",
    "### Create a DataFrame for predict output from ANN\n",
    "ANN_output = pd.DataFrame(classifier1.predict(Xs), index=Xs.index).rename(mapper=decoder_yNN, axis=1)\n",
    "### Add identifier to the column names\n",
    "ANN_output.columns = ['ANN_' + name for name in ANN_output.columns]\n",
    "\n",
    "### Create a DataFrame for predict proba output from RandomForest\n",
    "rdf_output = pd.DataFrame(rdf.predict_proba(X), columns=rdf.classes_, index=X.index)\n",
    "### Add identifier to the column names\n",
    "rdf_output.columns = ['rdf_' + name for name in rdf_output.columns]\n",
    "\n",
    "### Combine both outputs of the train data\n",
    "combined_output_train = ANN_output.merge(rdf_output, how='left', left_index=True, right_index=True).copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create derivative features for ANN and rdf for the KAGGLE TEST data\n",
    "\n",
    "### Create a DataFrame for predict output from ANN\n",
    "ANN_output = pd.DataFrame(classifier1.predict(kaggle_test_std), index=kaggle_test_std.index).rename(mapper=decoder_yNN, axis=1)\n",
    "### Add identifier to the column names\n",
    "ANN_output.columns = ['ANN_' + name for name in ANN_output.columns]\n",
    "\n",
    "### Create a DataFrame for predict proba output from RandomForest\n",
    "rdf_output = pd.DataFrame(rdf.predict_proba(kaggle_test), columns=rdf.classes_, index=kaggle_test.index)\n",
    "### Add identifier to the column names\n",
    "rdf_output.columns = ['rdf_' + name for name in rdf_output.columns]\n",
    "\n",
    "combined_output_kaggle = ANN_output.merge(rdf_output, how='left', left_index=True, right_index=True).copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=10, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Fit combined data through RandomForestClassifier\n",
    "rdf_combined = RandomForestClassifier(n_jobs=10, n_estimators=100, random_state=0, class_weight='balanced')\n",
    "rdf_combined.fit(combined_output_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_more_guesses = find_top_choices(pd.DataFrame(rdf_combined.predict_proba(combined_output_kaggle), columns=rdf_combined.classes_, index=combined_output_kaggle.index), threshold=1)\n",
    "submit_to_kaggle(rdf_more_guesses, 'Combined_rdfV2', '2nd try combined data with rdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning:\n",
      "\n",
      "Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=24, units=12, kernel_initializer=\"uniform\")`\n",
      "\n",
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning:\n",
      "\n",
      "Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=12, kernel_initializer=\"uniform\")`\n",
      "\n",
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning:\n",
      "\n",
      "Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=12, kernel_initializer=\"uniform\")`\n",
      "\n",
      "C:\\Users\\PaulY\\Anaconda3\\envs\\NLPTensorEnv\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning:\n",
      "\n",
      "The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 35s - loss: 0.8908 - categorical_accuracy: 0.7038\n",
      "Epoch 2/50\n",
      " - 36s - loss: 0.7459 - categorical_accuracy: 0.7589\n",
      "Epoch 3/50\n",
      " - 24s - loss: 0.7083 - categorical_accuracy: 0.7662\n",
      "Epoch 4/50\n",
      " - 27s - loss: 0.6805 - categorical_accuracy: 0.7722\n",
      "Epoch 5/50\n",
      " - 35s - loss: 0.6743 - categorical_accuracy: 0.7727\n",
      "Epoch 6/50\n",
      " - 31s - loss: 0.6714 - categorical_accuracy: 0.7732\n",
      "Epoch 7/50\n",
      " - 31s - loss: 0.6640 - categorical_accuracy: 0.7751\n",
      "Epoch 8/50\n",
      " - 39s - loss: 0.6522 - categorical_accuracy: 0.7763\n",
      "Epoch 9/50\n",
      " - 36s - loss: 0.6484 - categorical_accuracy: 0.7765\n",
      "Epoch 10/50\n",
      " - 26s - loss: 0.6456 - categorical_accuracy: 0.7770\n",
      "Epoch 11/50\n",
      " - 27s - loss: 0.6415 - categorical_accuracy: 0.7780\n",
      "Epoch 12/50\n",
      " - 25s - loss: 0.6365 - categorical_accuracy: 0.7783\n",
      "Epoch 13/50\n",
      " - 24s - loss: 0.6347 - categorical_accuracy: 0.7783\n",
      "Epoch 14/50\n",
      " - 24s - loss: 0.6338 - categorical_accuracy: 0.7789\n",
      "Epoch 15/50\n",
      " - 37s - loss: 0.6325 - categorical_accuracy: 0.7789\n",
      "Epoch 16/50\n",
      " - 38s - loss: 0.6314 - categorical_accuracy: 0.7786\n",
      "Epoch 17/50\n",
      " - 31s - loss: 0.6300 - categorical_accuracy: 0.7791\n",
      "Epoch 18/50\n",
      " - 35s - loss: 0.6285 - categorical_accuracy: 0.7792\n",
      "Epoch 19/50\n",
      " - 25s - loss: 0.6266 - categorical_accuracy: 0.7793\n",
      "Epoch 20/50\n",
      " - 24s - loss: 0.6249 - categorical_accuracy: 0.7801\n",
      "Epoch 21/50\n",
      " - 26s - loss: 0.6239 - categorical_accuracy: 0.7798\n",
      "Epoch 22/50\n",
      " - 33s - loss: 0.6233 - categorical_accuracy: 0.7796\n",
      "Epoch 23/50\n",
      " - 25s - loss: 0.6226 - categorical_accuracy: 0.7799\n",
      "Epoch 24/50\n",
      " - 24s - loss: 0.6224 - categorical_accuracy: 0.7801\n",
      "Epoch 25/50\n",
      " - 30s - loss: 0.6221 - categorical_accuracy: 0.7797\n",
      "Epoch 26/50\n",
      " - 37s - loss: 0.6217 - categorical_accuracy: 0.7804\n",
      "Epoch 27/50\n",
      " - 36s - loss: 0.6214 - categorical_accuracy: 0.7804\n",
      "Epoch 28/50\n",
      " - 35s - loss: 0.6214 - categorical_accuracy: 0.7806\n",
      "Epoch 29/50\n",
      " - 38s - loss: 0.6208 - categorical_accuracy: 0.7804\n",
      "Epoch 30/50\n",
      " - 30s - loss: 0.6207 - categorical_accuracy: 0.7803\n",
      "Epoch 31/50\n",
      " - 33s - loss: 0.6203 - categorical_accuracy: 0.7809\n",
      "Epoch 32/50\n",
      " - 25s - loss: 0.6202 - categorical_accuracy: 0.7809\n",
      "Epoch 33/50\n",
      " - 37s - loss: 0.6203 - categorical_accuracy: 0.7809\n",
      "Epoch 34/50\n",
      " - 36s - loss: 0.6199 - categorical_accuracy: 0.7811\n",
      "Epoch 35/50\n",
      " - 37s - loss: 0.6198 - categorical_accuracy: 0.7810\n",
      "Epoch 36/50\n",
      " - 25s - loss: 0.6193 - categorical_accuracy: 0.7813\n",
      "Epoch 37/50\n",
      " - 25s - loss: 0.6192 - categorical_accuracy: 0.7810\n",
      "Epoch 38/50\n",
      " - 32s - loss: 0.6191 - categorical_accuracy: 0.7816\n",
      "Epoch 39/50\n",
      " - 39s - loss: 0.6188 - categorical_accuracy: 0.7812\n",
      "Epoch 40/50\n",
      " - 31s - loss: 0.6190 - categorical_accuracy: 0.7816\n",
      "Epoch 41/50\n",
      " - 36s - loss: 0.6186 - categorical_accuracy: 0.7809\n",
      "Epoch 42/50\n",
      " - 41s - loss: 0.6184 - categorical_accuracy: 0.7817\n",
      "Epoch 43/50\n",
      " - 38s - loss: 0.6185 - categorical_accuracy: 0.7816\n",
      "Epoch 44/50\n",
      " - 32s - loss: 0.6183 - categorical_accuracy: 0.7819\n",
      "Epoch 45/50\n",
      " - 25s - loss: 0.6181 - categorical_accuracy: 0.7819\n",
      "Epoch 46/50\n",
      " - 25s - loss: 0.6179 - categorical_accuracy: 0.7814\n",
      "Epoch 47/50\n",
      " - 26s - loss: 0.6180 - categorical_accuracy: 0.7816\n",
      "Epoch 48/50\n",
      " - 34s - loss: 0.6177 - categorical_accuracy: 0.7821\n",
      "Epoch 49/50\n",
      " - 25s - loss: 0.6177 - categorical_accuracy: 0.7819\n",
      "Epoch 50/50\n",
      " - 25s - loss: 0.6174 - categorical_accuracy: 0.7820\n"
     ]
    }
   ],
   "source": [
    "classifierC = Sequential()\n",
    "classifierC.add(Dense(output_dim = 12, init = 'uniform', activation = 'relu', input_dim = combined_output_train.shape[1]))\n",
    "classifierC.add(Dense(output_dim = 12, init = 'uniform', activation = 'relu'))\n",
    "classifierC.add(Dense(output_dim = 12, init = 'uniform', activation = 'sigmoid'))\n",
    "classifierC.compile(optimizer = 'adam', loss= 'categorical_crossentropy', metrics = [keras.metrics.categorical_accuracy])\n",
    "classifierC.fit(combined_output_train, yNN, nb_epoch = 50, batch_size= 16, class_weight='balanced', verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 959k/959k [00:08<00:00, 110kB/s]\n"
     ]
    }
   ],
   "source": [
    "submit_to_kaggle(pd.DataFrame([decoder_yNN[val] for val in classifierC.predict_classes(combined_output_kaggle)], columns=['country'], index=kaggle_test_std.index), 'ANN combined result', '2nd try combined ANN model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4.68M/4.68M [00:07<00:00, 632kB/s]\n"
     ]
    }
   ],
   "source": [
    "### Score is actually noticeably worse with a score of 0.66\n",
    "### Try submitting more guesses for each user\n",
    "combined_ANN_output_df = pd.DataFrame(classifierC.predict_proba(combined_output_kaggle), index=combined_output_kaggle.index).rename(mapper=decoder_yNN, axis=1)\n",
    "ANN_more_guesses = find_best_choices(combined_ANN_output_df, threshold=1)\n",
    "submit_to_kaggle(ANN_more_guesses, 'ANN combined result', '2nd try combined ANN model with more guesses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    138447\n",
       "2     53654\n",
       "5         4\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'History' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-428-6d3e68ac89e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'History' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, classifier.predict_classes(X_test), labels= [1,0])\n",
    "print (classification_report(y_test, classifier.predict_classes(X_test)))\n",
    "cm = pd.DataFrame(cm, index=['True_High','True_Low'], columns = ['Pred_High','Pred_Low'])\n",
    "\n",
    "### Find and Print AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, pd.DataFrame(classifier.predict_proba(X_test)).iloc[:,0])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC: {}'.format(roc_auc))\n",
    "\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
